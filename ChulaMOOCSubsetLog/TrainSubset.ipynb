{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainSubset.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9am1OZbHezos","colab_type":"text"},"source":["#Task \n","Predict ว่าคนหนึ่งๆจะเรียนจบคอร์สหนึ่งๆหรือไม่\n","#Dataset (df2)\n","  Feature ทุุุกอย่างเหมือนอีกแบบหนึ่งยกเว้น\n","*   avg_score \n","*   quiz_submit\n","*   quiz_open\n","\n","ผมไม่ได้เอา record ทั้งหมดมาทำ feature แต่ตัดเอา record ครึ่งนึงมาทำ feature\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"aQLwwB_Hp-hx","colab_type":"code","outputId":"a6cfc40e-ebda-46ca-832d-7a10d88a434e","executionInfo":{"status":"ok","timestamp":1580825984116,"user_tz":-420,"elapsed":13360,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix\n","import lightgbm as lgbm\n","from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense,Dropout\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from xgboost import XGBClassifier\n","from sklearn import svm\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import GridSearchCV\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nKCN0BMftZ0Q","colab_type":"code","outputId":"e48f705e-83e4-4843-ccba-315d912d6155","executionInfo":{"status":"error","timestamp":1580825968121,"user_tz":-420,"elapsed":87,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"source":["#df2=pd.read_csv(\"dfhalf.csv\")\n","df2=pd.read_csv(\"20191112_CUMOOC_Data/df13.csv\")\n","df2=df2.drop(columns=[\"all_score\",\"nub\",\"cv_uid\",\"cv_cid\",\"UC\"])\n","df2\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-14d9151136c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20191112_CUMOOC_Data/df13.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"all_score\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"nub\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cv_uid\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cv_cid\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"UC\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","metadata":{"id":"qezDprQL8iVF","colab_type":"code","colab":{}},"source":["df2 = shuffle(df2)\n","X_train2=pd.DataFrame(df2)\n","Y_train2=X_train2[\"label\"]\n","X_train2=X_train2.drop(columns=[\"label\"])\n","Layer1DF=pd.DataFrame(df2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azG5Nf9xW26g","colab_type":"text"},"source":["\n","#ลองเอา dataset มา Train ก่อน"]},{"cell_type":"code","metadata":{"id":"0kkTQ4fxVKzl","colab_type":"code","outputId":"e8bb5d23-085d-4a1b-c3e1-e9541dd19371","executionInfo":{"status":"ok","timestamp":1580664019429,"user_tz":-420,"elapsed":2031,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["lg=LogisticRegression()\n","lg.fit(X_train2[:85000],Y_train2[:85000])\n","k=lg.score(X_train2[85000:], Y_train2[85000:])\n","print(k)\n","lgy=lg.predict(X_train2[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8611769513690233\n","0.7626506024096386 0.4925111845944369 0.598510814324548\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_kgqrPIZWJs","colab_type":"code","outputId":"612851cb-86b5-412b-e668-a889b7ba5591","executionInfo":{"status":"ok","timestamp":1580378083101,"user_tz":-420,"elapsed":1174120,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def build_nn():\n","  Kclass=Sequential()\n","  Kclass.add(Dense(15,activation='tanh',input_dim=6))\n","  Kclass.add(Dropout(0.2))\n","  Kclass.add(Dense(1,activation='sigmoid'))\n","  Kclass.compile(optimizer=\"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n","  return Kclass\n","KRlayer1=KerasClassifier(build_fn=build_nn,epochs=100,batch_size=10)\n","KRlayer1.fit(X_train2[:85000],Y_train2[:85000])\n","k=KRlayer1.score(X_train2[85000:], Y_train2[85000:])\n","print(k)\n","lgy=KRlayer1.predict(X_train2[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/100\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","85000/85000 [==============================] - 13s 148us/step - loss: 0.2659 - acc: 0.8766\n","Epoch 2/100\n","85000/85000 [==============================] - 12s 139us/step - loss: 0.2347 - acc: 0.8898\n","Epoch 3/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2302 - acc: 0.8939\n","Epoch 4/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2295 - acc: 0.8934\n","Epoch 5/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2270 - acc: 0.8951\n","Epoch 6/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2272 - acc: 0.8948\n","Epoch 7/100\n","85000/85000 [==============================] - 12s 139us/step - loss: 0.2261 - acc: 0.8962\n","Epoch 8/100\n","85000/85000 [==============================] - 13s 150us/step - loss: 0.2252 - acc: 0.8962\n","Epoch 9/100\n","85000/85000 [==============================] - 12s 146us/step - loss: 0.2255 - acc: 0.8962\n","Epoch 10/100\n","85000/85000 [==============================] - 12s 139us/step - loss: 0.2250 - acc: 0.8975\n","Epoch 11/100\n","85000/85000 [==============================] - 12s 139us/step - loss: 0.2244 - acc: 0.8972\n","Epoch 12/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2240 - acc: 0.8989\n","Epoch 13/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2236 - acc: 0.8984\n","Epoch 14/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2231 - acc: 0.8982\n","Epoch 15/100\n","85000/85000 [==============================] - 12s 142us/step - loss: 0.2235 - acc: 0.8981\n","Epoch 16/100\n","85000/85000 [==============================] - 12s 143us/step - loss: 0.2223 - acc: 0.8991\n","Epoch 17/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2224 - acc: 0.8994\n","Epoch 18/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2225 - acc: 0.8997\n","Epoch 19/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2221 - acc: 0.8989\n","Epoch 20/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2212 - acc: 0.8996\n","Epoch 21/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2210 - acc: 0.9008\n","Epoch 22/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2212 - acc: 0.9005\n","Epoch 23/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2210 - acc: 0.9008\n","Epoch 24/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2201 - acc: 0.9018\n","Epoch 25/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2192 - acc: 0.9023\n","Epoch 26/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2186 - acc: 0.9021\n","Epoch 27/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2189 - acc: 0.9018\n","Epoch 28/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2179 - acc: 0.9019\n","Epoch 29/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2189 - acc: 0.9021\n","Epoch 30/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2188 - acc: 0.9021\n","Epoch 31/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2192 - acc: 0.9031\n","Epoch 32/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2183 - acc: 0.9030\n","Epoch 33/100\n","85000/85000 [==============================] - 12s 139us/step - loss: 0.2177 - acc: 0.9025\n","Epoch 34/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2185 - acc: 0.9025\n","Epoch 35/100\n","85000/85000 [==============================] - 12s 144us/step - loss: 0.2181 - acc: 0.9036\n","Epoch 36/100\n","85000/85000 [==============================] - 12s 139us/step - loss: 0.2183 - acc: 0.9022\n","Epoch 37/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2180 - acc: 0.9030\n","Epoch 38/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2169 - acc: 0.9037\n","Epoch 39/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2180 - acc: 0.9032\n","Epoch 40/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2175 - acc: 0.9034\n","Epoch 41/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2176 - acc: 0.9028\n","Epoch 42/100\n","85000/85000 [==============================] - 12s 146us/step - loss: 0.2174 - acc: 0.9038\n","Epoch 43/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2171 - acc: 0.9036\n","Epoch 44/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2168 - acc: 0.9039\n","Epoch 45/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2167 - acc: 0.9033\n","Epoch 46/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2173 - acc: 0.9037\n","Epoch 47/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2169 - acc: 0.9040\n","Epoch 48/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2167 - acc: 0.9043\n","Epoch 49/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2157 - acc: 0.9047\n","Epoch 50/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2161 - acc: 0.9040\n","Epoch 51/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2162 - acc: 0.9041\n","Epoch 52/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2168 - acc: 0.9036\n","Epoch 53/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2163 - acc: 0.9036\n","Epoch 54/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2166 - acc: 0.9040\n","Epoch 55/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2158 - acc: 0.9043\n","Epoch 56/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2157 - acc: 0.9042\n","Epoch 57/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2160 - acc: 0.9049\n","Epoch 58/100\n","85000/85000 [==============================] - 12s 135us/step - loss: 0.2156 - acc: 0.9045\n","Epoch 59/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2167 - acc: 0.9038\n","Epoch 60/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2159 - acc: 0.9048\n","Epoch 61/100\n","85000/85000 [==============================] - 12s 135us/step - loss: 0.2162 - acc: 0.9046\n","Epoch 62/100\n","85000/85000 [==============================] - 12s 146us/step - loss: 0.2163 - acc: 0.9039\n","Epoch 63/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2165 - acc: 0.9049\n","Epoch 64/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2152 - acc: 0.9054\n","Epoch 65/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2146 - acc: 0.9048\n","Epoch 66/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2162 - acc: 0.9043\n","Epoch 67/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2150 - acc: 0.9054\n","Epoch 68/100\n","85000/85000 [==============================] - 12s 140us/step - loss: 0.2153 - acc: 0.9050\n","Epoch 69/100\n","85000/85000 [==============================] - 12s 142us/step - loss: 0.2153 - acc: 0.9053\n","Epoch 70/100\n","85000/85000 [==============================] - 11s 133us/step - loss: 0.2158 - acc: 0.9045\n","Epoch 71/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2150 - acc: 0.9057\n","Epoch 72/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2148 - acc: 0.9057\n","Epoch 73/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2150 - acc: 0.9053\n","Epoch 74/100\n","85000/85000 [==============================] - 12s 135us/step - loss: 0.2147 - acc: 0.9058\n","Epoch 75/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2154 - acc: 0.9048\n","Epoch 76/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2157 - acc: 0.9050\n","Epoch 77/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2163 - acc: 0.9042\n","Epoch 78/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2149 - acc: 0.9051\n","Epoch 79/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2150 - acc: 0.9061\n","Epoch 80/100\n","85000/85000 [==============================] - 12s 138us/step - loss: 0.2144 - acc: 0.9062\n","Epoch 81/100\n","85000/85000 [==============================] - 11s 134us/step - loss: 0.2143 - acc: 0.9064\n","Epoch 82/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2149 - acc: 0.9060\n","Epoch 83/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2153 - acc: 0.9061\n","Epoch 84/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2145 - acc: 0.9061\n","Epoch 85/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2144 - acc: 0.9068\n","Epoch 86/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2140 - acc: 0.9066\n","Epoch 87/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2145 - acc: 0.9062\n","Epoch 88/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2144 - acc: 0.9056\n","Epoch 89/100\n","85000/85000 [==============================] - 13s 152us/step - loss: 0.2147 - acc: 0.9059\n","Epoch 90/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2133 - acc: 0.9069\n","Epoch 91/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2146 - acc: 0.9061\n","Epoch 92/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2146 - acc: 0.9067\n","Epoch 93/100\n","85000/85000 [==============================] - 12s 135us/step - loss: 0.2141 - acc: 0.9069\n","Epoch 94/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2137 - acc: 0.9064\n","Epoch 95/100\n","85000/85000 [==============================] - 12s 145us/step - loss: 0.2135 - acc: 0.9064\n","Epoch 96/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2144 - acc: 0.9065\n","Epoch 97/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2143 - acc: 0.9066\n","Epoch 98/100\n","85000/85000 [==============================] - 12s 137us/step - loss: 0.2136 - acc: 0.9059\n","Epoch 99/100\n","85000/85000 [==============================] - 12s 136us/step - loss: 0.2134 - acc: 0.9069\n","Epoch 100/100\n","85000/85000 [==============================] - 11s 135us/step - loss: 0.2147 - acc: 0.9057\n","24470/24470 [==============================] - 1s 61us/step\n","0.9154066125446307\n","0.7651773533424284 0.8662162162162163 0.812567910177472\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_b_H-XMIZnJX","colab_type":"code","outputId":"7e81fa92-9cd8-4402-e2ad-9a7b201cabaf","executionInfo":{"status":"ok","timestamp":1580665142133,"user_tz":-420,"elapsed":2276,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["lg=lgbm.LGBMClassifier()\n","lg.fit(X_train2[:85000],Y_train2[:85000])\n","k=lg.score(X_train2[85000:], Y_train2[85000:])\n","print(k)\n","lgy=lg.predict(X_train2[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9312627707396812\n","0.8030488873313475 0.8914608052907994 0.8449483775811211\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fpTXLEzB-gNI","colab_type":"code","outputId":"337d3a3b-afd4-44a2-9791-18b617c05c61","executionInfo":{"status":"ok","timestamp":1580665127510,"user_tz":-420,"elapsed":2446,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X_train3=X_train2.drop(columns=[\"G1\",\"G2\",\"G3\"])\n","lg=lgbm.LGBMClassifier()\n","lg.fit(X_train3[:85000],Y_train2[:85000])\n","k=lg.score(X_train3[85000:], Y_train2[85000:])\n","print(k)\n","lgy=lg.predict(X_train3[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9270535349407437\n","0.7891109579600276 0.8908772612332231 0.8369118318867064\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iLFOCjyMHDTo","colab_type":"text"},"source":["# ที่ต่างจากแบบแรกคือ LogisticRegression มี f1 score ที่ลดลงมากเหลือประมาณ 50% และทุกโมเดลมี accuracy ลดลง"]},{"cell_type":"markdown","metadata":{"id":"jjbIlcb9alS-","colab_type":"text"},"source":["#Stacking"]},{"cell_type":"code","metadata":{"id":"XOJR4LPzb4W_","colab_type":"code","outputId":"195b58e0-18a1-4fa1-8bed-69058342b2d6","executionInfo":{"status":"ok","timestamp":1580702500273,"user_tz":-420,"elapsed":2246,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["LRlayer1=LogisticRegression()\n","LRlayer1.fit(X_train2,Y_train2)\n","out=LRlayer1.predict_proba(X_train2)\n","Layer1DF[\"LRProb\"]=out[:,1]\n","Layer1DF"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>enroll_count</th>\n","      <th>complete_count</th>\n","      <th>quiz_submit</th>\n","      <th>quiz_open</th>\n","      <th>avg_score</th>\n","      <th>did_watch</th>\n","      <th>G1</th>\n","      <th>G2</th>\n","      <th>G3</th>\n","      <th>LRProb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>90845</th>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.405607</td>\n","    </tr>\n","    <tr>\n","      <th>107929</th>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.023758</td>\n","    </tr>\n","    <tr>\n","      <th>105919</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.431703</td>\n","    </tr>\n","    <tr>\n","      <th>98670</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.018819</td>\n","    </tr>\n","    <tr>\n","      <th>78284</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>16</td>\n","      <td>10</td>\n","      <td>0.857143</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.866308</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>19548</th>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.016881</td>\n","    </tr>\n","    <tr>\n","      <th>85810</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.337558</td>\n","    </tr>\n","    <tr>\n","      <th>99614</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.387808</td>\n","    </tr>\n","    <tr>\n","      <th>13579</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.036337</td>\n","    </tr>\n","    <tr>\n","      <th>84411</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>0.800000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.560560</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>109470 rows × 11 columns</p>\n","</div>"],"text/plain":["        label  enroll_count  complete_count  quiz_submit  ...  G1  G2  G3    LRProb\n","90845       0             7               3            1  ...   0   1   0  0.405607\n","107929      0             8               2            0  ...   0   1   0  0.023758\n","105919      0             5               3            0  ...   0   1   0  0.431703\n","98670       0             5               0            0  ...   1   0   0  0.018819\n","78284       1             1               1           16  ...   0   0   0  0.866308\n","...       ...           ...             ...          ...  ...  ..  ..  ..       ...\n","19548       0             8               1            0  ...   0   1   0  0.016881\n","85810       0             0               0            0  ...   1   0   0  0.337558\n","99614       1             1               1            0  ...   0   0   0  0.387808\n","13579       0             0               0            0  ...   1   0   0  0.036337\n","84411       1             0               0            5  ...   1   0   0  0.560560\n","\n","[109470 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"HLqM3TAEdGAF","colab_type":"code","colab":{}},"source":["lg=lgbm.LGBMClassifier()\n","lg.fit(X_train2,Y_train2)\n","out=lg.predict_proba(X_train2)\n","Layer1DF[\"LGProb\"]=out[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWhdS287o7J3","colab_type":"code","colab":{}},"source":["ada=AdaBoostClassifier()\n","ada.fit(X_train2,Y_train2)\n","out=ada.predict_proba(X_train2)\n","Layer1DF[\"ADAProb\"]=out[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHYcPGXvpnG4","colab_type":"code","colab":{}},"source":["ext=ExtraTreesClassifier()\n","ext.fit(X_train2,Y_train2)\n","out=ext.predict_proba(X_train2)\n","Layer1DF[\"EXTProb\"]=out[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CBZwef2rXvh","colab_type":"code","colab":{}},"source":["kn=KNeighborsClassifier()\n","kn.fit(X_train2,Y_train2)\n","out=kn.predict_proba(X_train2)\n","Layer1DF[\"KNProb\"]=out[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tp-4d8I8GUKb","colab_type":"code","colab":{}},"source":["rf=RandomForestClassifier()\n","rf.fit(X_train2, Y_train2)\n","out=rf.predict_proba(X_train2)\n","Layer1DF[\"RFProb\"]=out[:,1]\n","#OO=rf.feature_importances_.reshape(1,6)\n","#for i in range(OO.shape[1]):\n","#  IM[i]+=OO[0,i]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VtF5lW1jwTqs","outputId":"51770fd4-8299-4184-db2a-2772b395a988","executionInfo":{"status":"ok","timestamp":1580702550266,"user_tz":-420,"elapsed":52169,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["xgb = XGBClassifier()\n","xgb.fit(X_train2, Y_train2)\n","print(xgb.score(X_train2,Y_train2))\n","out=xgb.predict_proba(X_train2)\n","Layer1DF[\"XGProb\"]=out[:,1]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.924947474193843\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"soRr4NFXwg0V","colab_type":"code","outputId":"af8cc23d-1a66-4226-de5a-3cc22bfb53a7","executionInfo":{"status":"ok","timestamp":1580703969424,"user_tz":-420,"elapsed":1370595,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def build_nn():\n","  Kclass=Sequential()\n","  Kclass.add(Dense(15,activation='tanh',input_dim=9))\n","  Kclass.add(Dropout(0.2))\n","  Kclass.add(Dense(1,activation='sigmoid'))\n","  Kclass.compile(optimizer=\"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n","  return Kclass\n","KRlayer1=KerasClassifier(build_fn=build_nn,epochs=100,batch_size=10)\n","KRlayer1.fit(X_train2,Y_train2)\n","out=KRlayer1.predict_proba(X_train2)\n","Layer1DF[\"KRProb\"]=out[:,1]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/100\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2503 - acc: 0.8812\n","Epoch 2/100\n","109470/109470 [==============================] - 15s 134us/step - loss: 0.2203 - acc: 0.8955\n","Epoch 3/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2141 - acc: 0.8997\n","Epoch 4/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2109 - acc: 0.9019\n","Epoch 5/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2098 - acc: 0.9025\n","Epoch 6/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2092 - acc: 0.9029\n","Epoch 7/100\n","109470/109470 [==============================] - 14s 126us/step - loss: 0.2092 - acc: 0.9041\n","Epoch 8/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2089 - acc: 0.9030\n","Epoch 9/100\n","109470/109470 [==============================] - 14s 126us/step - loss: 0.2076 - acc: 0.9031\n","Epoch 10/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2082 - acc: 0.9044\n","Epoch 11/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2076 - acc: 0.9043\n","Epoch 12/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2064 - acc: 0.9049\n","Epoch 13/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2070 - acc: 0.9043\n","Epoch 14/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2063 - acc: 0.9049\n","Epoch 15/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2063 - acc: 0.9047\n","Epoch 16/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2059 - acc: 0.9058\n","Epoch 17/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2061 - acc: 0.9045\n","Epoch 18/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2057 - acc: 0.9052\n","Epoch 19/100\n","109470/109470 [==============================] - 13s 120us/step - loss: 0.2053 - acc: 0.9063\n","Epoch 20/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2051 - acc: 0.9054\n","Epoch 21/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2048 - acc: 0.9064\n","Epoch 22/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2043 - acc: 0.9070\n","Epoch 23/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2040 - acc: 0.9068\n","Epoch 24/100\n","109470/109470 [==============================] - 14s 123us/step - loss: 0.2046 - acc: 0.9063\n","Epoch 25/100\n","109470/109470 [==============================] - 15s 133us/step - loss: 0.2044 - acc: 0.9063\n","Epoch 26/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2037 - acc: 0.9075\n","Epoch 27/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2039 - acc: 0.9072\n","Epoch 28/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2040 - acc: 0.9072\n","Epoch 29/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2039 - acc: 0.9070\n","Epoch 30/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2029 - acc: 0.9073\n","Epoch 31/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2035 - acc: 0.9067\n","Epoch 32/100\n","109470/109470 [==============================] - 14s 131us/step - loss: 0.2033 - acc: 0.9076\n","Epoch 33/100\n","109470/109470 [==============================] - 14s 131us/step - loss: 0.2037 - acc: 0.9072\n","Epoch 34/100\n","109470/109470 [==============================] - 15s 138us/step - loss: 0.2030 - acc: 0.9077\n","Epoch 35/100\n","109470/109470 [==============================] - 16s 144us/step - loss: 0.2030 - acc: 0.9073\n","Epoch 36/100\n","109470/109470 [==============================] - 14s 132us/step - loss: 0.2035 - acc: 0.9068\n","Epoch 37/100\n","109470/109470 [==============================] - 15s 136us/step - loss: 0.2029 - acc: 0.9083\n","Epoch 38/100\n","109470/109470 [==============================] - 15s 136us/step - loss: 0.2030 - acc: 0.9069\n","Epoch 39/100\n","109470/109470 [==============================] - 15s 136us/step - loss: 0.2029 - acc: 0.9079\n","Epoch 40/100\n","109470/109470 [==============================] - 14s 131us/step - loss: 0.2035 - acc: 0.9067\n","Epoch 41/100\n","109470/109470 [==============================] - 15s 136us/step - loss: 0.2029 - acc: 0.9077\n","Epoch 42/100\n","109470/109470 [==============================] - 15s 133us/step - loss: 0.2024 - acc: 0.9082\n","Epoch 43/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2021 - acc: 0.9084\n","Epoch 44/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2020 - acc: 0.9087\n","Epoch 45/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2019 - acc: 0.9088\n","Epoch 46/100\n","109470/109470 [==============================] - 14s 126us/step - loss: 0.2026 - acc: 0.9083\n","Epoch 47/100\n","109470/109470 [==============================] - 14s 126us/step - loss: 0.2028 - acc: 0.9081\n","Epoch 48/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2029 - acc: 0.9085\n","Epoch 49/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2017 - acc: 0.9088\n","Epoch 50/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2024 - acc: 0.9090\n","Epoch 51/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2023 - acc: 0.9087\n","Epoch 52/100\n","109470/109470 [==============================] - 14s 123us/step - loss: 0.2018 - acc: 0.9078\n","Epoch 53/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2017 - acc: 0.9089\n","Epoch 54/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2017 - acc: 0.9087\n","Epoch 55/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2018 - acc: 0.9092\n","Epoch 56/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2015 - acc: 0.9092\n","Epoch 57/100\n","109470/109470 [==============================] - 14s 123us/step - loss: 0.2021 - acc: 0.9091\n","Epoch 58/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2019 - acc: 0.9085\n","Epoch 59/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2014 - acc: 0.9096\n","Epoch 60/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2008 - acc: 0.9093\n","Epoch 61/100\n","109470/109470 [==============================] - 14s 123us/step - loss: 0.2015 - acc: 0.9098\n","Epoch 62/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2012 - acc: 0.9096\n","Epoch 63/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2018 - acc: 0.9099\n","Epoch 64/100\n","109470/109470 [==============================] - 14s 123us/step - loss: 0.2008 - acc: 0.9099\n","Epoch 65/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.2002 - acc: 0.9100\n","Epoch 66/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2016 - acc: 0.9090\n","Epoch 67/100\n","109470/109470 [==============================] - 13s 120us/step - loss: 0.2010 - acc: 0.9093\n","Epoch 68/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2008 - acc: 0.9097\n","Epoch 69/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2010 - acc: 0.9101\n","Epoch 70/100\n","109470/109470 [==============================] - 14s 130us/step - loss: 0.2001 - acc: 0.9096\n","Epoch 71/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2001 - acc: 0.9098\n","Epoch 72/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2000 - acc: 0.9097\n","Epoch 73/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.2008 - acc: 0.9102\n","Epoch 74/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1995 - acc: 0.9101\n","Epoch 75/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2001 - acc: 0.9100\n","Epoch 76/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2011 - acc: 0.9089\n","Epoch 77/100\n","109470/109470 [==============================] - 14s 126us/step - loss: 0.2011 - acc: 0.9098\n","Epoch 78/100\n","109470/109470 [==============================] - 13s 120us/step - loss: 0.1995 - acc: 0.9103\n","Epoch 79/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.1992 - acc: 0.9104\n","Epoch 80/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2004 - acc: 0.9098\n","Epoch 81/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1994 - acc: 0.9109\n","Epoch 82/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2005 - acc: 0.9098\n","Epoch 83/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2007 - acc: 0.9102\n","Epoch 84/100\n","109470/109470 [==============================] - 13s 120us/step - loss: 0.2004 - acc: 0.9096\n","Epoch 85/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.2000 - acc: 0.9098\n","Epoch 86/100\n","109470/109470 [==============================] - 14s 125us/step - loss: 0.2000 - acc: 0.9098\n","Epoch 87/100\n","109470/109470 [==============================] - 13s 120us/step - loss: 0.2001 - acc: 0.9102\n","Epoch 88/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1998 - acc: 0.9105\n","Epoch 89/100\n","109470/109470 [==============================] - 13s 119us/step - loss: 0.1997 - acc: 0.9104\n","Epoch 90/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1996 - acc: 0.9104\n","Epoch 91/100\n","109470/109470 [==============================] - 13s 123us/step - loss: 0.2002 - acc: 0.9104\n","Epoch 92/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1993 - acc: 0.9108\n","Epoch 93/100\n","109470/109470 [==============================] - 14s 127us/step - loss: 0.1998 - acc: 0.9105\n","Epoch 94/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1988 - acc: 0.9113\n","Epoch 95/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1997 - acc: 0.9097\n","Epoch 96/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.1992 - acc: 0.9108\n","Epoch 97/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1995 - acc: 0.9104\n","Epoch 98/100\n","109470/109470 [==============================] - 13s 121us/step - loss: 0.1998 - acc: 0.9100\n","Epoch 99/100\n","109470/109470 [==============================] - 14s 124us/step - loss: 0.1992 - acc: 0.9108\n","Epoch 100/100\n","109470/109470 [==============================] - 13s 122us/step - loss: 0.1993 - acc: 0.9106\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DQV_2uk9UQMV","colab_type":"code","colab":{}},"source":["#IM=[[-val,key] for key,val in IM.items()]\n","#IM.sort()\n","#print(IM)\n","#Layer1DF=Layer1DF.drop(columns=[\"complete_count\",\"did_watch\"])\n","#ayer1DF[Layer1DF[\"label\"]==1]\n","#Layer1DF"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKnPW4-9HdM5","colab_type":"code","colab":{}},"source":["Layer1DF.to_csv(\"dflayer1half_sex.csv\",index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBRAjQw64HJh","colab_type":"code","colab":{}},"source":["Layer1DF=pd.read_csv(\"dflayer1half_sex.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fv4PrpMZuuwn","colab_type":"code","outputId":"47ffa680-56aa-4716-b539-1116bbb2039e","executionInfo":{"status":"ok","timestamp":1580704029140,"user_tz":-420,"elapsed":2743,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["lg=lgbm.LGBMClassifier()\n","Y=Layer1DF[\"label\"]\n","MM=Layer1DF.drop(columns=[\"label\"])\n","lg.fit(MM[:85000],Y[:85000])\n","k=lg.score(MM[85000:], Y[85000:])\n","k2=lg.score(MM[:85000], Y[:85000])\n","print(k2,k)\n","lgy=lg.predict(MM[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9560235294117647 0.9487127094401308\n","0.8623352165725047 0.8973153047227121 0.8794775761067896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hghJRpUqzcpV","colab_type":"code","colab":{}},"source":["lg=LogisticRegression()\n","Y=OK[\"label\"]\n","MM=OK.drop(columns=[\"label\"])\n","lg.fit(MM[:80000],Y[:80000])\n","k=lg.score(MM[80000:], Y[80000:])\n","print(k)\n","lgy=lg.predict(MM[80000:])\n","tn, fp, fn, tp = confusion_matrix(Y[80000:],lgy).ravel()\n","print(tn,fp,fn,tp)\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ZCczANMcakz","colab_type":"text"},"source":["# หลังการ stack จะได้ accuracy ประมาณ 95% น้อยกว่าแบบแรก (97%)\n"]}]}