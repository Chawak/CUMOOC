{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ChulaMOOCStacking.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9am1OZbHezos","colab_type":"text"},"source":["#Task \n","Predict ว่าคนหนึ่งๆจะเรียนจบคอร์สหนึ่งๆหรือไม่\n","#Dataset (df2)\n","  ใน Dataframe แต่ละแถวจะมี cv_uid,cv_cid ซ้ำกันได้ แต่จะไม่ซ้ำกันทั้งสองตัว ผมใช้ 6 Feature คือ\n","*   complete_count = จำนวนคอร์สที่เรียนจบ ก่อนจะมา enroll คอร์สนี้\n","*   enroll_count= จำนวนคอร์สที่ enroll ก่อนจะมา enroll คอร์สนี้\n","*   avg_score = คะแนนเฉลี่ย (คิดเป็นร้อยละ/100) ของคอร์สนี้\n","*   quiz_submit=จำนวนครั้งที่ส่ง quiz คอร์สนี้\n","*   quiz_open=จำนวนครั้งที่เปิด quiz คอร์สนี้\n","*   did_watch=เช็คว่าเคยดูวิดีโอของคอร์สนี้หรือไม่ ถ้าเคยจะเป็น 1 ถ้าไม่เคยจะเป็น 0\n","* G1,G2,G3=boolean features G1=1 เมื่อนักเรียนเป็น Male G2=1 เมื่อนักเรียนเป็น Female G3=1 เมื่อนักเรียนไม่ใช่ทั้ง Male และ Female\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fOa9xOkPTvu2","colab_type":"text"},"source":["ผมทำ Stacking 2 Layer\n","Layer 1 : \n","\n","\n","*   Logistic Regression\n","*   LigthGBM\n","*   Adaboost\n","*   Extratree\n","*   K-nearest neighbor\n","*   Random Forest\n","*   XGBoost\n","*   Neural Network\n","\n","#ขั้นตอน\n","\n","\n","\n","1.   เอา Data ทั้งหมด Train ให้แต่ละ model แล้วเอา Predict Probability ของแต่ละ model รวมกับ feature เดิม เอามาทำเป็น Dataset ใหม่ \n","\n","\n","\n","\n","Layer2 : ใช้ตัวเดียวคือ LogisticRegression ใช้ Train กับ Dataset ใหม่ที่ได้จาก Layer1 "]},{"cell_type":"code","metadata":{"id":"aQLwwB_Hp-hx","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix\n","import lightgbm as lgbm\n","from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense,Dropout\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from xgboost import XGBClassifier\n","from sklearn import svm\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MnmFTqLKxQH","colab_type":"code","colab":{}},"source":["student=pd.read_csv(\"05_cvocp_user_info_masked.csv\")\n","df2[\"G1\"]=0\n","df2[\"G2\"]=0\n","df2[\"G3\"]=0\n","for idx,row in student.iterrows():\n","  if row[\"gender\"]==\"Male\":\n","    df2.loc[df2[\"cv_uid\"]==row[\"cv_uid\"],[\"G1\"]]=1\n","  elif row[\"gender\"]==\"Female\":\n","    df2.loc[df2[\"cv_uid\"]==row[\"cv_uid\"],[\"G2\"]]=1\n","  else:\n","    df2.loc[df2[\"cv_uid\"]==row[\"cv_uid\"],[\"G3\"]]=1\n","  print(idx)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKCN0BMftZ0Q","colab_type":"code","colab":{}},"source":["#df2=pd.read_csv(\"20191112_CUMOOC_Data/dfneweraaisus.csv\")\n","df2=pd.read_csv(\"dfneweraaisus.csv\")\n","df2=df2.drop(columns=[\"all_score\",\"nub\",\"cv_uid\",\"cv_cid\"])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qezDprQL8iVF","colab_type":"code","colab":{}},"source":["df2 = shuffle(df2)\n","X_train2=pd.DataFrame(df2)\n","Y_train2=X_train2[\"label\"]\n","XOK=X_train2[85000:]\n","XOK=XOK[XOK[\"label\"]==1]\n","YOK=XOK[\"label\"]\n","XOK=XOK.drop(columns=[\"label\"])\n","X_train2=X_train2.drop(columns=[\"label\"])\n","Layer1DF=pd.DataFrame(df2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Th0-zB19UoUy","colab_type":"code","colab":{}},"source":["df2.to_csv(\"df_sex.csv\",index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azG5Nf9xW26g","colab_type":"text"},"source":["\n","#ลองเอา dataset มา Train ก่อน"]},{"cell_type":"code","metadata":{"id":"0kkTQ4fxVKzl","colab_type":"code","outputId":"da796829-8837-4803-d467-0de8ea775420","executionInfo":{"status":"ok","timestamp":1580705770741,"user_tz":-420,"elapsed":2838,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["lg=LogisticRegression()\n","lg.fit(X_train2[:85000],Y_train2[:85000])\n","k=lg.score(X_train2[85000:], Y_train2[85000:])\n","print(k)\n","lgy=lg.predict(X_train2[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8918675929709848\n","0.763271162123386 0.7140939597315437 0.737864077669903\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_kgqrPIZWJs","colab_type":"code","outputId":"db2c70b8-5ed9-4b8c-af61-566a1cd02701","executionInfo":{"status":"ok","timestamp":1579509234394,"user_tz":-420,"elapsed":2512182,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def build_nn():\n","  Kclass=Sequential()\n","  Kclass.add(Dense(15,activation='tanh',input_dim=6))\n","  Kclass.add(Dropout(0.2))\n","  Kclass.add(Dense(1,activation='sigmoid'))\n","  Kclass.compile(optimizer=\"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n","  return Kclass\n","KRlayer1=KerasClassifier(build_fn=build_nn,epochs=100,batch_size=10)\n","KRlayer1.fit(X_train2[:85000],Y_train2[:85000])\n","k=KRlayer1.score(X_train2[85000:], Y_train2[85000:])\n","print(k)\n","lgy=KRlayer1.predict(X_train2[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/100\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","85000/85000 [==============================] - 35s 407us/step - loss: 0.2663 - acc: 0.8775\n","Epoch 2/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2288 - acc: 0.8953\n","Epoch 3/100\n","85000/85000 [==============================] - 25s 297us/step - loss: 0.2224 - acc: 0.8980\n","Epoch 4/100\n","85000/85000 [==============================] - 26s 303us/step - loss: 0.2172 - acc: 0.9008\n","Epoch 5/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2156 - acc: 0.9022\n","Epoch 6/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2141 - acc: 0.9016\n","Epoch 7/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.2133 - acc: 0.9019\n","Epoch 8/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2118 - acc: 0.9020\n","Epoch 9/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2121 - acc: 0.9025\n","Epoch 10/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2108 - acc: 0.9026\n","Epoch 11/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.2113 - acc: 0.9024\n","Epoch 12/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2096 - acc: 0.9029\n","Epoch 13/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2089 - acc: 0.9038\n","Epoch 14/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2098 - acc: 0.9031\n","Epoch 15/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2080 - acc: 0.9035\n","Epoch 16/100\n","85000/85000 [==============================] - 26s 302us/step - loss: 0.2098 - acc: 0.9029\n","Epoch 17/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.2083 - acc: 0.9048\n","Epoch 18/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2069 - acc: 0.9050\n","Epoch 19/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2077 - acc: 0.9054\n","Epoch 20/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2063 - acc: 0.9054\n","Epoch 21/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2057 - acc: 0.9055\n","Epoch 22/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.2058 - acc: 0.9066\n","Epoch 23/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2063 - acc: 0.9046\n","Epoch 24/100\n","85000/85000 [==============================] - 24s 287us/step - loss: 0.2055 - acc: 0.9059\n","Epoch 25/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2058 - acc: 0.9055\n","Epoch 26/100\n","85000/85000 [==============================] - 25s 289us/step - loss: 0.2051 - acc: 0.9061\n","Epoch 27/100\n","85000/85000 [==============================] - 25s 289us/step - loss: 0.2062 - acc: 0.9054\n","Epoch 28/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2059 - acc: 0.9054\n","Epoch 29/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2050 - acc: 0.9057\n","Epoch 30/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2056 - acc: 0.9055\n","Epoch 31/100\n","85000/85000 [==============================] - 24s 288us/step - loss: 0.2043 - acc: 0.9060\n","Epoch 32/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2047 - acc: 0.9074\n","Epoch 33/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2043 - acc: 0.9061\n","Epoch 34/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2033 - acc: 0.9068\n","Epoch 35/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2036 - acc: 0.9065\n","Epoch 36/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2039 - acc: 0.9070\n","Epoch 37/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2038 - acc: 0.9066\n","Epoch 38/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2037 - acc: 0.9073\n","Epoch 39/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2024 - acc: 0.9076\n","Epoch 40/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2040 - acc: 0.9065\n","Epoch 41/100\n","85000/85000 [==============================] - 26s 302us/step - loss: 0.2022 - acc: 0.9075\n","Epoch 42/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2018 - acc: 0.9077\n","Epoch 43/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2020 - acc: 0.9078\n","Epoch 44/100\n","85000/85000 [==============================] - 25s 291us/step - loss: 0.2027 - acc: 0.9070\n","Epoch 45/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2023 - acc: 0.9081\n","Epoch 46/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2031 - acc: 0.9071\n","Epoch 47/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2016 - acc: 0.9090\n","Epoch 48/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2016 - acc: 0.9079\n","Epoch 49/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2014 - acc: 0.9083\n","Epoch 50/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2011 - acc: 0.9078\n","Epoch 51/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2016 - acc: 0.9083\n","Epoch 52/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2019 - acc: 0.9081\n","Epoch 53/100\n","85000/85000 [==============================] - 25s 297us/step - loss: 0.2013 - acc: 0.9086\n","Epoch 54/100\n","85000/85000 [==============================] - 25s 297us/step - loss: 0.2019 - acc: 0.9073\n","Epoch 55/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2006 - acc: 0.9083\n","Epoch 56/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.2016 - acc: 0.9079\n","Epoch 57/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.2000 - acc: 0.9097\n","Epoch 58/100\n","85000/85000 [==============================] - 25s 297us/step - loss: 0.2013 - acc: 0.9088\n","Epoch 59/100\n","85000/85000 [==============================] - 25s 291us/step - loss: 0.2006 - acc: 0.9086\n","Epoch 60/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.1998 - acc: 0.9094\n","Epoch 61/100\n","85000/85000 [==============================] - 25s 299us/step - loss: 0.2010 - acc: 0.9076\n","Epoch 62/100\n","85000/85000 [==============================] - 24s 288us/step - loss: 0.2007 - acc: 0.9083\n","Epoch 63/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.2007 - acc: 0.9080\n","Epoch 64/100\n","85000/85000 [==============================] - 25s 289us/step - loss: 0.2003 - acc: 0.9089\n","Epoch 65/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.1997 - acc: 0.9088\n","Epoch 66/100\n","85000/85000 [==============================] - 27s 312us/step - loss: 0.2011 - acc: 0.9076\n","Epoch 67/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2011 - acc: 0.9079\n","Epoch 68/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2014 - acc: 0.9075\n","Epoch 69/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2008 - acc: 0.9088\n","Epoch 70/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.1997 - acc: 0.9088\n","Epoch 71/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.1998 - acc: 0.9082\n","Epoch 72/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.2018 - acc: 0.9067\n","Epoch 73/100\n","85000/85000 [==============================] - 25s 291us/step - loss: 0.1997 - acc: 0.9096\n","Epoch 74/100\n","85000/85000 [==============================] - 25s 297us/step - loss: 0.2003 - acc: 0.9083\n","Epoch 75/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.1992 - acc: 0.9095\n","Epoch 76/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.1991 - acc: 0.9094\n","Epoch 77/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.1993 - acc: 0.9095\n","Epoch 78/100\n","85000/85000 [==============================] - 26s 301us/step - loss: 0.1993 - acc: 0.9088\n","Epoch 79/100\n","85000/85000 [==============================] - 25s 289us/step - loss: 0.2004 - acc: 0.9077\n","Epoch 80/100\n","85000/85000 [==============================] - 25s 291us/step - loss: 0.1991 - acc: 0.9097\n","Epoch 81/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.1997 - acc: 0.9091\n","Epoch 82/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.1997 - acc: 0.9084\n","Epoch 83/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.1992 - acc: 0.9087\n","Epoch 84/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2001 - acc: 0.9086\n","Epoch 85/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.1999 - acc: 0.9083\n","Epoch 86/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.2010 - acc: 0.9081\n","Epoch 87/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.2001 - acc: 0.9079\n","Epoch 88/100\n","85000/85000 [==============================] - 25s 290us/step - loss: 0.2000 - acc: 0.9086\n","Epoch 89/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.1987 - acc: 0.9095\n","Epoch 90/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.1993 - acc: 0.9090\n","Epoch 91/100\n","85000/85000 [==============================] - 26s 304us/step - loss: 0.1986 - acc: 0.9087\n","Epoch 92/100\n","85000/85000 [==============================] - 25s 294us/step - loss: 0.1995 - acc: 0.9089\n","Epoch 93/100\n","85000/85000 [==============================] - 25s 298us/step - loss: 0.1995 - acc: 0.9099\n","Epoch 94/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.1994 - acc: 0.9088\n","Epoch 95/100\n","85000/85000 [==============================] - 25s 292us/step - loss: 0.1981 - acc: 0.9104\n","Epoch 96/100\n","85000/85000 [==============================] - 25s 295us/step - loss: 0.1995 - acc: 0.9096\n","Epoch 97/100\n","85000/85000 [==============================] - 25s 296us/step - loss: 0.1990 - acc: 0.9096\n","Epoch 98/100\n","85000/85000 [==============================] - 25s 297us/step - loss: 0.1987 - acc: 0.9100\n","Epoch 99/100\n","85000/85000 [==============================] - 25s 291us/step - loss: 0.1987 - acc: 0.9104\n","Epoch 100/100\n","85000/85000 [==============================] - 25s 293us/step - loss: 0.1982 - acc: 0.9103\n","24470/24470 [==============================] - 3s 113us/step\n","0.9096035885966745\n","0.7733382434174185 0.8106543138390272 0.79155672823219\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_b_H-XMIZnJX","colab_type":"code","outputId":"2e3a5730-8264-4a92-b14b-2f7b70295a9a","executionInfo":{"status":"ok","timestamp":1580920478722,"user_tz":-420,"elapsed":4364,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["lg=XGBClassifier()\n","lg.fit(X_train2[:85000],Y_train2[:85000])\n","k=lg.score(X_train2[85000:], Y_train2[85000:])\n","print(k)\n","lgy=lg.predict(X_train2[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y_train2[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)\n","print(classification_report(Y_train2[85000:], lgy))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9409889660809154\n","0.8408342480790341 0.8889748549323018 0.864234674689733\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.95      0.96     19300\n","           1       0.84      0.89      0.86      5170\n","\n","    accuracy                           0.94     24470\n","   macro avg       0.91      0.92      0.91     24470\n","weighted avg       0.94      0.94      0.94     24470\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RMrUabrJLLDj","colab_type":"code","outputId":"e11048f2-648c-4a07-d24d-f16bbcca1db2","executionInfo":{"status":"ok","timestamp":1580920201968,"user_tz":-420,"elapsed":4231,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["lg=XGBClassifier()\n","lg.fit(X_train2[:85000],Y_train2[:85000])\n","k=lg.score(XOK, YOK)\n","print(k)\n","lgy=lg.predict(XOK)\n","tn, fp, fn, tp = confusion_matrix(YOK,lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8889748549323018\n","1.0 0.8889748549323018 0.9412246569731723\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jjbIlcb9alS-","colab_type":"text"},"source":["#Stacking"]},{"cell_type":"code","metadata":{"id":"XOJR4LPzb4W_","colab_type":"code","outputId":"7a46da56-fafc-4595-ec76-f0acde25c67c","executionInfo":{"status":"ok","timestamp":1580705929083,"user_tz":-420,"elapsed":2680,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["LRlayer1=LogisticRegression()\n","LRlayer1.fit(X_train2,Y_train2)\n","out=LRlayer1.predict_proba(X_train2)\n","Layer1DF[\"LRProb\"]=out[:,1]\n","Layer1DF"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>enroll_count</th>\n","      <th>complete_count</th>\n","      <th>quiz_submit</th>\n","      <th>quiz_open</th>\n","      <th>avg_score</th>\n","      <th>did_watch</th>\n","      <th>G1</th>\n","      <th>G2</th>\n","      <th>G3</th>\n","      <th>LRProb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>68257</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.137625</td>\n","    </tr>\n","    <tr>\n","      <th>88523</th>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.003002</td>\n","    </tr>\n","    <tr>\n","      <th>37395</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.008142</td>\n","    </tr>\n","    <tr>\n","      <th>99090</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>16</td>\n","      <td>0.7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.586932</td>\n","    </tr>\n","    <tr>\n","      <th>45208</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.013391</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7786</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.011173</td>\n","    </tr>\n","    <tr>\n","      <th>36783</th>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.011409</td>\n","    </tr>\n","    <tr>\n","      <th>17257</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.008142</td>\n","    </tr>\n","    <tr>\n","      <th>68156</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.156000</td>\n","    </tr>\n","    <tr>\n","      <th>26728</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.012109</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>109470 rows × 11 columns</p>\n","</div>"],"text/plain":["       label  enroll_count  complete_count  quiz_submit  ...  G1  G2  G3    LRProb\n","68257      0             3               0            1  ...   0   1   0  0.137625\n","88523      0            17               1            0  ...   0   1   0  0.003002\n","37395      0             5               0            0  ...   0   1   0  0.008142\n","99090      1             2               2           10  ...   0   0   0  0.586932\n","45208      0             1               0            1  ...   0   1   0  0.013391\n","...      ...           ...             ...          ...  ...  ..  ..  ..       ...\n","7786       0             2               0            0  ...   0   1   0  0.011173\n","36783      0            10               3            0  ...   1   0   0  0.011409\n","17257      0             5               0            0  ...   0   1   0  0.008142\n","68156      0             1               0            1  ...   1   0   0  0.156000\n","26728      0             0               0            0  ...   0   0   1  0.012109\n","\n","[109470 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"HLqM3TAEdGAF","colab_type":"code","colab":{}},"source":["lg=lgbm.LGBMClassifier()\n","lg.fit(X_train2,Y_train2)\n","out=lg.predict_proba(X_train2)\n","Layer1DF[\"LGProb\"]=out[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWhdS287o7J3","colab_type":"code","colab":{}},"source":["ada=AdaBoostClassifier()\n","ada.fit(X_train2,Y_train2)\n","out=ada.predict_proba(X_train2)\n","Layer1DF[\"ADAProb\"]=out[:,1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHYcPGXvpnG4","colab_type":"code","colab":{}},"source":["ext=ExtraTreesClassifier()\n","ext.fit(X_train2,Y_train2)\n","out=ext.predict_proba(X_train2)\n","Layer1DF[\"EXTProb\"]=out[:,1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CBZwef2rXvh","colab_type":"code","colab":{}},"source":["kn=KNeighborsClassifier()\n","kn.fit(X_train2,Y_train2)\n","out=kn.predict_proba(X_train2)\n","Layer1DF[\"KNProb\"]=out[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tp-4d8I8GUKb","colab_type":"code","colab":{}},"source":["rf=RandomForestClassifier()\n","rf.fit(X_train2, Y_train2)\n","out=rf.predict_proba(X_train2)\n","Layer1DF[\"RFProb\"]=out[:,1]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VtF5lW1jwTqs","outputId":"67b4a087-529b-4677-9aa8-a3799548097b","executionInfo":{"status":"ok","timestamp":1580705966539,"user_tz":-420,"elapsed":40094,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["xgb = XGBClassifier()\n","xgb.fit(X_train2, Y_train2)\n","print(xgb.score(X_train2,Y_train2))\n","out=xgb.predict_proba(X_train2)\n","Layer1DF[\"XGProb\"]=out[:,1]\n","#OO=xgb.feature_importances_.reshape(1,6)\n","#for i in range(OO.shape[1]):\n","#  IM[i]+=OO[0,i]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9419384306202613\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"soRr4NFXwg0V","colab_type":"code","outputId":"7b5744cf-8086-47fc-9ff7-6982385b8255","executionInfo":{"status":"ok","timestamp":1580710707041,"user_tz":-420,"elapsed":4586617,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def build_nn():\n","  Kclass=Sequential()\n","  Kclass.add(Dense(15,activation='tanh',input_dim=9))\n","  Kclass.add(Dropout(0.2))\n","  Kclass.add(Dense(1,activation='sigmoid'))\n","  Kclass.compile(optimizer=\"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n","  return Kclass\n","KRlayer1=KerasClassifier(build_fn=build_nn,epochs=100,batch_size=10)\n","KRlayer1.fit(X_train2,Y_train2)\n","out=KRlayer1.predict_proba(X_train2)\n","Layer1DF[\"KRProb\"]=out[:,1]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/100\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","109470/109470 [==============================] - 56s 512us/step - loss: 0.2445 - acc: 0.8854\n","Epoch 2/100\n","109470/109470 [==============================] - 46s 421us/step - loss: 0.2077 - acc: 0.9029\n","Epoch 3/100\n","109470/109470 [==============================] - 47s 428us/step - loss: 0.1984 - acc: 0.9070\n","Epoch 4/100\n","109470/109470 [==============================] - 47s 426us/step - loss: 0.1915 - acc: 0.9100\n","Epoch 5/100\n","109470/109470 [==============================] - 46s 421us/step - loss: 0.1873 - acc: 0.9126\n","Epoch 6/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1855 - acc: 0.9143\n","Epoch 7/100\n","109470/109470 [==============================] - 46s 422us/step - loss: 0.1851 - acc: 0.9145\n","Epoch 8/100\n","109470/109470 [==============================] - 46s 422us/step - loss: 0.1849 - acc: 0.9147\n","Epoch 9/100\n","109470/109470 [==============================] - 46s 421us/step - loss: 0.1841 - acc: 0.9153\n","Epoch 10/100\n","109470/109470 [==============================] - 46s 422us/step - loss: 0.1843 - acc: 0.9156\n","Epoch 11/100\n","109470/109470 [==============================] - 46s 422us/step - loss: 0.1833 - acc: 0.9166\n","Epoch 12/100\n","109470/109470 [==============================] - 46s 422us/step - loss: 0.1822 - acc: 0.9170\n","Epoch 13/100\n","109470/109470 [==============================] - 46s 419us/step - loss: 0.1809 - acc: 0.9171\n","Epoch 14/100\n","109470/109470 [==============================] - 46s 423us/step - loss: 0.1792 - acc: 0.9184\n","Epoch 15/100\n","109470/109470 [==============================] - 46s 419us/step - loss: 0.1808 - acc: 0.9176\n","Epoch 16/100\n","109470/109470 [==============================] - 46s 419us/step - loss: 0.1815 - acc: 0.9170\n","Epoch 17/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1813 - acc: 0.9178\n","Epoch 18/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1797 - acc: 0.9181\n","Epoch 19/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1800 - acc: 0.9180\n","Epoch 20/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1794 - acc: 0.9181\n","Epoch 21/100\n","109470/109470 [==============================] - 46s 423us/step - loss: 0.1794 - acc: 0.9187\n","Epoch 22/100\n","109470/109470 [==============================] - 46s 419us/step - loss: 0.1790 - acc: 0.9184\n","Epoch 23/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1780 - acc: 0.9191\n","Epoch 24/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1790 - acc: 0.9192\n","Epoch 25/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1783 - acc: 0.9197\n","Epoch 26/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1787 - acc: 0.9186\n","Epoch 27/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1770 - acc: 0.9199\n","Epoch 28/100\n","109470/109470 [==============================] - 47s 425us/step - loss: 0.1777 - acc: 0.9189\n","Epoch 29/100\n","109470/109470 [==============================] - 46s 418us/step - loss: 0.1778 - acc: 0.9195\n","Epoch 30/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1776 - acc: 0.9201\n","Epoch 31/100\n","109470/109470 [==============================] - 47s 426us/step - loss: 0.1770 - acc: 0.9194\n","Epoch 32/100\n","109470/109470 [==============================] - 47s 425us/step - loss: 0.1776 - acc: 0.9194\n","Epoch 33/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1769 - acc: 0.9192\n","Epoch 34/100\n","109470/109470 [==============================] - 46s 421us/step - loss: 0.1775 - acc: 0.9194\n","Epoch 35/100\n","109470/109470 [==============================] - 47s 425us/step - loss: 0.1764 - acc: 0.9206\n","Epoch 36/100\n","109470/109470 [==============================] - 46s 419us/step - loss: 0.1749 - acc: 0.9212\n","Epoch 37/100\n","109470/109470 [==============================] - 46s 423us/step - loss: 0.1753 - acc: 0.9204\n","Epoch 38/100\n","109470/109470 [==============================] - 46s 422us/step - loss: 0.1757 - acc: 0.9199\n","Epoch 39/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1751 - acc: 0.9209\n","Epoch 40/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1759 - acc: 0.9204\n","Epoch 41/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1754 - acc: 0.9204\n","Epoch 42/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1747 - acc: 0.9213\n","Epoch 43/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1760 - acc: 0.9204\n","Epoch 44/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1747 - acc: 0.9207\n","Epoch 45/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1744 - acc: 0.9207\n","Epoch 46/100\n","109470/109470 [==============================] - 45s 412us/step - loss: 0.1749 - acc: 0.9207\n","Epoch 47/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1749 - acc: 0.9206\n","Epoch 48/100\n","109470/109470 [==============================] - 46s 419us/step - loss: 0.1742 - acc: 0.9217\n","Epoch 49/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1746 - acc: 0.9213\n","Epoch 50/100\n","109470/109470 [==============================] - 45s 412us/step - loss: 0.1741 - acc: 0.9217\n","Epoch 51/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1748 - acc: 0.9210\n","Epoch 52/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1745 - acc: 0.9208\n","Epoch 53/100\n","109470/109470 [==============================] - 45s 412us/step - loss: 0.1739 - acc: 0.9212\n","Epoch 54/100\n","109470/109470 [==============================] - 45s 412us/step - loss: 0.1733 - acc: 0.9215\n","Epoch 55/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1742 - acc: 0.9202\n","Epoch 56/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1740 - acc: 0.9210\n","Epoch 57/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1743 - acc: 0.9203\n","Epoch 58/100\n","109470/109470 [==============================] - 45s 410us/step - loss: 0.1732 - acc: 0.9221\n","Epoch 59/100\n","109470/109470 [==============================] - 45s 412us/step - loss: 0.1734 - acc: 0.9217\n","Epoch 60/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1734 - acc: 0.9222\n","Epoch 61/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1737 - acc: 0.9213\n","Epoch 62/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1729 - acc: 0.9217\n","Epoch 63/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1732 - acc: 0.9215\n","Epoch 64/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1722 - acc: 0.9226\n","Epoch 65/100\n","109470/109470 [==============================] - 45s 412us/step - loss: 0.1728 - acc: 0.9211\n","Epoch 66/100\n","109470/109470 [==============================] - 45s 411us/step - loss: 0.1730 - acc: 0.9219\n","Epoch 67/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1728 - acc: 0.9211\n","Epoch 68/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1733 - acc: 0.9213\n","Epoch 69/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1725 - acc: 0.9225\n","Epoch 70/100\n","109470/109470 [==============================] - 46s 421us/step - loss: 0.1718 - acc: 0.9225\n","Epoch 71/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1734 - acc: 0.9207\n","Epoch 72/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1723 - acc: 0.9224\n","Epoch 73/100\n","109470/109470 [==============================] - 46s 418us/step - loss: 0.1724 - acc: 0.9222\n","Epoch 74/100\n","109470/109470 [==============================] - 46s 416us/step - loss: 0.1724 - acc: 0.9224\n","Epoch 75/100\n","109470/109470 [==============================] - 46s 418us/step - loss: 0.1725 - acc: 0.9218\n","Epoch 76/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1719 - acc: 0.9223\n","Epoch 77/100\n","109470/109470 [==============================] - 46s 420us/step - loss: 0.1712 - acc: 0.9228\n","Epoch 78/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1728 - acc: 0.9211\n","Epoch 79/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1722 - acc: 0.9228\n","Epoch 80/100\n","109470/109470 [==============================] - 46s 418us/step - loss: 0.1714 - acc: 0.9227\n","Epoch 81/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1716 - acc: 0.9227\n","Epoch 82/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1723 - acc: 0.9225\n","Epoch 83/100\n","109470/109470 [==============================] - 46s 418us/step - loss: 0.1722 - acc: 0.9221\n","Epoch 84/100\n","109470/109470 [==============================] - 46s 421us/step - loss: 0.1719 - acc: 0.9230\n","Epoch 85/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1715 - acc: 0.9217\n","Epoch 86/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1716 - acc: 0.9225\n","Epoch 87/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1704 - acc: 0.9228\n","Epoch 88/100\n","109470/109470 [==============================] - 45s 415us/step - loss: 0.1719 - acc: 0.9221\n","Epoch 89/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1717 - acc: 0.9217\n","Epoch 90/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1714 - acc: 0.9229\n","Epoch 91/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1712 - acc: 0.9223\n","Epoch 92/100\n","109470/109470 [==============================] - 45s 413us/step - loss: 0.1710 - acc: 0.9227\n","Epoch 93/100\n","109470/109470 [==============================] - 45s 410us/step - loss: 0.1718 - acc: 0.9219\n","Epoch 94/100\n","109470/109470 [==============================] - 45s 409us/step - loss: 0.1714 - acc: 0.9225\n","Epoch 95/100\n","109470/109470 [==============================] - 45s 414us/step - loss: 0.1718 - acc: 0.9228\n","Epoch 96/100\n","109470/109470 [==============================] - 46s 417us/step - loss: 0.1714 - acc: 0.9227\n","Epoch 97/100\n","109470/109470 [==============================] - 45s 411us/step - loss: 0.1716 - acc: 0.9228\n","Epoch 98/100\n","109470/109470 [==============================] - 45s 410us/step - loss: 0.1711 - acc: 0.9231\n","Epoch 99/100\n","109470/109470 [==============================] - 45s 411us/step - loss: 0.1720 - acc: 0.9221\n","Epoch 100/100\n","109470/109470 [==============================] - 45s 411us/step - loss: 0.1715 - acc: 0.9225\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LODCr0mweeFp","colab_type":"text"},"source":["คิด Feature Importance จะได้ว่า complete_count กับ did_watch สำคัญน้อยสุดเลยตัดทิ้งไป"]},{"cell_type":"code","metadata":{"id":"DQV_2uk9UQMV","colab_type":"code","colab":{}},"source":["#IM=[[-val,key] for key,val in IM.items()]\n","#IM.sort()\n","#print(IM)\n","#Layer1DF=Layer1DF.drop(columns=[\"complete_count\",\"did_watch\"])\n","#Layer1DF[Layer1DF[\"label\"]==1]\n","#Layer1DF"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKnPW4-9HdM5","colab_type":"code","colab":{}},"source":["Layer1DF.to_csv(\"dflayer1_sex.csv\",index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fIy30qqLa5EQ","colab_type":"text"},"source":["#Layer2"]},{"cell_type":"markdown","metadata":{"id":"FULgAmCsb7y2","colab_type":"text"},"source":["Layer2 : ใช้ตัวเดียวคือ LogisticRegression (ผมลองใช้ตัวอื่นแล้วไม่ต่างกัน ได้ accuracy ประมาณ 0.97 f1-score ประมาณ 0.92) \n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"XBRAjQw64HJh","colab_type":"code","colab":{}},"source":["Layer1DF=pd.read_csv(\"dflayer1_sex.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fv4PrpMZuuwn","colab_type":"code","outputId":"38add0e3-8108-4a8e-a9d3-310a3a7fbd77","executionInfo":{"status":"ok","timestamp":1580716044599,"user_tz":-420,"elapsed":3376,"user":{"displayName":"Chawakorn Phiantham","photoUrl":"","userId":"05119468888744169723"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["lg=LogisticRegression()\n","Y=Layer1DF[\"label\"]\n","MM=Layer1DF.drop(columns=[\"label\"])\n","lg.fit(MM[:85000],Y[:85000])\n","k=lg.score(MM[85000:], Y[85000:])\n","print(k)\n","lgy=lg.predict(MM[85000:])\n","tn, fp, fn, tp = confusion_matrix(Y[85000:],lgy).ravel()\n","precision=tp/(tp+fp)\n","recall=tp/(tp+fn)\n","f1=2*precision*recall/(precision+recall)\n","print(precision,recall,f1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9738863914997957\n","0.9394928928159816 0.9378715244487057 0.9386815084924672\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7ZCczANMcakz","colab_type":"text"},"source":["ผมลอง Stack เพิ่มอีก 1 Layer แล้ว ผลลัพธ์ที่ได้ไม่ต่างกันผมเลย Stack แค่ 2 ชั้น \n","\n"]}]}